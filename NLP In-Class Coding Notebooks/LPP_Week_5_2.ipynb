{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Latent Semantic Analysis: How Many Topics? A Practical Guide\n",
        "\n",
        "## Today's Focus\n",
        "1. Calculate explained variance\n",
        "2. Create a scree plot\n",
        "3. Apply the elbow method\n",
        "4. Test our choice with real data"
      ],
      "metadata": {
        "id": "Z4ud250sCGM-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Load presidential speeches\n",
        "df = pd.read_excel(\"presidential_speeches_updated.xlsx\")\n",
        "print(f\"Loaded {len(df)} speeches from {df['speaker'].nunique()} speakers\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HFQQHGSWCGG5",
        "outputId": "8df81170-9d6e-4b30-ae43-5ef61a885a92"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded 20 speeches from 4 speakers\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")"
      ],
      "metadata": {
        "id": "tecrM74XENng"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a function to apply spaCy to a single text\n",
        "def process_lemma_spacy(text):\n",
        "\n",
        "    # Example: return a list of tokens\n",
        "\n",
        "# Apply the function to the 'speech'\n"
      ],
      "metadata": {
        "id": "0HFKswAOEPQY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 1: Create TF-IDF Matrix"
      ],
      "metadata": {
        "id": "g6HwW79yCGB-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert to TF-IDF\n",
        "vectorizer = TfidfVectorizer(stop_words='english',\n",
        "                             max_features=500,\n",
        "                             min_df = .25, # Removes terms that appear too infrequently, as % of manuscripts\n",
        "                             max_df = .75 # Removes terms that appear too frequently, as % of manuscripts\n",
        "                             )\n",
        "\n",
        "X_tfidf = vectorizer.fit_transform(df['speech_lemma'])\n",
        "print(f\"Matrix shape: {X_tfidf.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w1i-rdzWCF6H",
        "outputId": "a75a1a2f-eab4-4953-d7a4-02ae169b550f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Matrix shape: (20, 500)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 2: Test Different Numbers of Topics\n",
        "\n",
        "We'll test 1-10 topics and see how much information each captures."
      ],
      "metadata": {
        "id": "FUb5I-sGCF1c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Test different numbers of topics\n",
        "max_topics = 10\n",
        "variances = []\n",
        "\n",
        "# Fit and calculate the total variance explained by each number of topics\n",
        "\n",
        "    # Fit LSA with n topics\n",
        "\n",
        "    # Store total variance explained\n",
        "\n",
        "    # Store total variance across"
      ],
      "metadata": {
        "id": "oPCyLj8nC4nI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 3: Variance Explained Plot\n",
        "\n",
        "Look for the number of topics where the plateau of variance explained occurs."
      ],
      "metadata": {
        "id": "DHJJDxRlC2-0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Instead of re-fitting for each n, fit once with max_topics\n",
        "\n",
        "\n",
        "# Individual explained variance ratios\n",
        "\n",
        "# Cumulative explained variance\n"
      ],
      "metadata": {
        "id": "s4tjqWo9DFRW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ---- Plot ----\n",
        "x = np.arange(1, max_topics + 1)\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(9,5))\n",
        "ax.bar(x, indiv_var, color=\"seagreen\", alpha=0.65, label=\"Individual Explained Variance\")\n",
        "ax.plot(x, cum_var, marker=\"o\", color=\"crimson\", linewidth=2, label=\"Cumulative Explained Variance\")\n",
        "\n",
        "# Labels above bars\n",
        "for xi, yi in zip(x, indiv_var):\n",
        "    ax.text(xi, yi + 0.01, f\"{yi*100:.1f}%\", ha=\"center\", va=\"bottom\", fontsize=9, color=\"seagreen\")\n",
        "\n",
        "# Labels on cumulative points\n",
        "for xi, yi in zip(x, cum_var):\n",
        "    ax.annotate(f\"{yi*100:.1f}%\", (xi, yi), textcoords=\"offset points\", xytext=(0,8),\n",
        "                ha=\"center\", fontsize=9, color=\"crimson\")\n",
        "\n",
        "ax.set_title(\"Explained Variance by Different Principal Components (Topics)\")\n",
        "ax.set_xlabel(\"Principal Components (Topics)\")\n",
        "ax.set_ylabel(\"Explained Variance\")\n",
        "ax.set_xticks(x)\n",
        "ax.set_ylim(0, 1.05)\n",
        "ax.grid(True, axis=\"y\", alpha=0.3)\n",
        "ax.legend(loc=\"upper left\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "xAWbwAOS__v2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 4: Comparing Topic Quality Across Different K Values\n",
        "\n",
        "Let's examine how well documents cluster with different numbers of topics and identify the most representative words for each topic.\n"
      ],
      "metadata": {
        "id": "XqGx8tkqHbZP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.1: Extract and Compare Top Words for Different K Values"
      ],
      "metadata": {
        "id": "EkMB_CtTHcve"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize empty list to store our results\n",
        "topics_with_words = []\n",
        "\n",
        "# How many top words to extract per topic\n",
        "n_words = 10\n",
        "\n",
        "# Get the vocabulary (feature names) from the vectorizer\n",
        "# This gives us the actual words corresponding to each column in our matrix\n",
        "feature_names = vectorizer.get_feature_names_out()\n",
        "\n",
        "# Iterate through each topic (component) from the SVD\n",
        "# topic_idx: 0, 1, 2, ... (topic number)\n",
        "# topic: 1D array of weights for all words in this topic (e.g. how much a word contributes to a topic)\n",
        "\n",
        "    # Sort and extract indices of top n-words for this topic\n",
        "\n",
        "    # Convert indices to actual words using feature_names\n",
        "\n",
        "    # Get the weight values for these top words\n",
        "\n",
        "    # Store everything in a dictionary for this topic"
      ],
      "metadata": {
        "id": "R741v9zKSncp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert above code into a function\n",
        "def get_top_words(svd_model, feature_names, n_words=10):\n",
        "    \"\"\"Extract top words for each topic\"\"\"\n",
        "    topics_with_words = []\n",
        "\n",
        "    for topic_idx, topic in enumerate(svd_model.components_):\n",
        "        # Get indices of top words\n",
        "        top_word_indices = topic.argsort()[::-1][:n_words]\n",
        "        top_words = [feature_names[i] for i in top_word_indices]\n",
        "\n",
        "        # Get their weights\n",
        "        weights = topic[top_word_indices]\n",
        "\n",
        "        topics_with_words.append({\n",
        "            'topic_num': topic_idx + 1,\n",
        "            'words': top_words,\n",
        "            'weights': weights\n",
        "        })\n",
        "\n",
        "    return topics_with_words"
      ],
      "metadata": {
        "id": "CGgoJa-EHqpE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compare top words for 3 to 7 topics\n",
        "k_values_to_compare = [3,4,5,6,7]\n",
        "\n",
        "# Loop through multiple values of k and compare their outputs in the get_top_words() function\n"
      ],
      "metadata": {
        "id": "k4VN4aL4VwOw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.2: Visualize Topic Coherence\n"
      ],
      "metadata": {
        "id": "Y0V-_gQFQQsu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a more detailed comparison for a selected number of topics\n",
        "selected_k = 4  # CHANGE BASED ON THE VARIABILITY EXPLAINED PLOT AND TOP WORDS ANALYSIS\n",
        "\n",
        "# Fit a new LSA/SVD Model\n",
        "svd_selected = TruncatedSVD(n_components=selected_k, random_state=42)\n",
        "doc_topics = svd_selected.fit_transform(X_tfidf)\n",
        "\n",
        "# Visualize topics dataframe\n",
        "pd.DataFrame(doc_topics)"
      ],
      "metadata": {
        "id": "NGn5GF97QOTC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a heatmap of top words per topic\n",
        "fig, axes = plt.subplots(1, selected_k, figsize=(15, 4))\n",
        "fig.suptitle(f'Top 10 Words per Topic (k={selected_k})', fontsize=14)\n",
        "\n",
        "topics = get_top_words(svd_selected, feature_names, n_words=10)\n",
        "\n",
        "for idx, (ax, topic) in enumerate(zip(axes, topics)):\n",
        "    # Create bar plot for each topic\n",
        "    words = topic['words'][::-1]  # Reverse for bottom-to-top display\n",
        "    weights = topic['weights'][::-1]\n",
        "\n",
        "    ax.barh(range(len(words)), weights, color=plt.cm.viridis(0.3 + idx * 0.15))\n",
        "    ax.set_yticks(range(len(words)))\n",
        "    ax.set_yticklabels(words, fontsize=9)\n",
        "    ax.set_xlabel('Weight', fontsize=10)\n",
        "    ax.set_title(f'Topic {idx + 1}', fontsize=11)\n",
        "    ax.grid(True, alpha=0.3, axis='x')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "vB19k--RaKZs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.4: Document Distribution Across Topics"
      ],
      "metadata": {
        "id": "p7HpZDRlmzjM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Show how documents are distributed across topics\n",
        "\n",
        "# Add dominant topic for each document\n",
        "\n",
        "# Count documents per dominant topic\n",
        "\n",
        "# Include speaker from original dataframe\n"
      ],
      "metadata": {
        "id": "4qidxLRgQMtI"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}