{"cells":[{"cell_type":"markdown","id":"a786c77c","metadata":{"id":"a786c77c"},"source":["# Introduction to Memory in LangChain and LangGraph\n","In LLM applications, the \"memory\" component is crucial to maintain conversation state. LangChain's memory module has evolved significantly. Modern LangChain provides several approaches depending on your needs—whether you want to store a full conversation, manage messages with middleware, or build agent-based systems with tools. Understanding these tradeoffs is essential when teaching how LLMs can maintain context over extended interactions.\n","\n","This notebook demonstrates various memory approaches in modern LangChain and LangGraph, such as:\n","- RunnableWithMessageHistory (for basic chains)\n","- LangGraph with checkpointers (for stateful workflows)\n","- Memory management strategies (trim, delete, summarize [on Thursday])"]},{"cell_type":"code","source":["# Install the required packages\n","!pip install langchain langchain-openai langgraph"],"metadata":{"id":"B-VohANHTeDl","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1762794978976,"user_tz":300,"elapsed":6267,"user":{"displayName":"David Smiley","userId":"04196970161563073370"}},"outputId":"083e226c-b2d7-4163-9428-80ff4fc8219d","collapsed":true},"id":"B-VohANHTeDl","execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: langchain in /usr/local/lib/python3.12/dist-packages (0.3.27)\n","Requirement already satisfied: langchain-openai in /usr/local/lib/python3.12/dist-packages (0.3.35)\n","Requirement already satisfied: langgraph in /usr/local/lib/python3.12/dist-packages (1.0.1)\n","Requirement already satisfied: langchain-core<1.0.0,>=0.3.72 in /usr/local/lib/python3.12/dist-packages (from langchain) (0.3.79)\n","Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.9 in /usr/local/lib/python3.12/dist-packages (from langchain) (0.3.11)\n","Requirement already satisfied: langsmith>=0.1.17 in /usr/local/lib/python3.12/dist-packages (from langchain) (0.4.40)\n","Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.12/dist-packages (from langchain) (2.11.10)\n","Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.12/dist-packages (from langchain) (2.0.44)\n","Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.12/dist-packages (from langchain) (2.32.5)\n","Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.12/dist-packages (from langchain) (6.0.3)\n","Requirement already satisfied: openai<3.0.0,>=1.104.2 in /usr/local/lib/python3.12/dist-packages (from langchain-openai) (1.109.1)\n","Requirement already satisfied: tiktoken<1.0.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from langchain-openai) (0.12.0)\n","Requirement already satisfied: langgraph-checkpoint<4.0.0,>=2.1.0 in /usr/local/lib/python3.12/dist-packages (from langgraph) (3.0.1)\n","Requirement already satisfied: langgraph-prebuilt<1.1.0,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langgraph) (1.0.1)\n","Requirement already satisfied: langgraph-sdk<0.3.0,>=0.2.2 in /usr/local/lib/python3.12/dist-packages (from langgraph) (0.2.9)\n","Requirement already satisfied: xxhash>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from langgraph) (3.6.0)\n","Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (8.5.0)\n","Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (1.33)\n","Requirement already satisfied: typing-extensions<5.0.0,>=4.7.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (4.15.0)\n","Requirement already satisfied: packaging<26.0.0,>=23.2.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (25.0)\n","Requirement already satisfied: ormsgpack>=1.12.0 in /usr/local/lib/python3.12/dist-packages (from langgraph-checkpoint<4.0.0,>=2.1.0->langgraph) (1.12.0)\n","Requirement already satisfied: httpx>=0.25.2 in /usr/local/lib/python3.12/dist-packages (from langgraph-sdk<0.3.0,>=0.2.2->langgraph) (0.28.1)\n","Requirement already satisfied: orjson>=3.10.1 in /usr/local/lib/python3.12/dist-packages (from langgraph-sdk<0.3.0,>=0.2.2->langgraph) (3.11.4)\n","Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.1.17->langchain) (1.0.0)\n","Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.1.17->langchain) (0.25.0)\n","Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from openai<3.0.0,>=1.104.2->langchain-openai) (4.11.0)\n","Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from openai<3.0.0,>=1.104.2->langchain-openai) (1.9.0)\n","Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from openai<3.0.0,>=1.104.2->langchain-openai) (0.11.1)\n","Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from openai<3.0.0,>=1.104.2->langchain-openai) (1.3.1)\n","Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.12/dist-packages (from openai<3.0.0,>=1.104.2->langchain-openai) (4.67.1)\n","Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n","Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.33.2)\n","Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.2)\n","Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2->langchain) (3.4.4)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2->langchain) (3.11)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2->langchain) (2.5.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2->langchain) (2025.10.5)\n","Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.12/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.2.4)\n","Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.12/dist-packages (from tiktoken<1.0.0,>=0.7.0->langchain-openai) (2024.11.6)\n","Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.2->langgraph) (1.0.9)\n","Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.2->langgraph) (0.16.0)\n","Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core<1.0.0,>=0.3.72->langchain) (3.0.0)\n"]}]},{"cell_type":"markdown","id":"1297dcd5","metadata":{"id":"1297dcd5"},"source":["## 1. RunnableWithMessageHistory (Basic Chains)\n","\n","Key Concepts:\n","- Session Management: Each conversation gets a unique session_id\n","- Automatic History: Messages are automatically stored and retrieved\n","- Simple API: Works with any LCEL chain (chain = prompt | llm)\n","\n","When to Use:\n","- Building simple chatbots without tools\n","- Short to medium-length conversations\n","- Rapid prototyping and demos\n","- Applications where full state control isn't needed\n","\n","When Not to Use:\n","- Very long conversations (risk of context overflow)\n","- Complex multi-step workflows\n","- Applications requiring custom state beyond messages\n","- Systems needing fine-grained state control\n","\n","Architecture:\n","User Input → Chain → LLM → Response\n","                ↓              ↓\n","        Session Store ← History Retrieved"]},{"cell_type":"markdown","source":["### 1.1 - Import Dependencies"],"metadata":{"id":"ucG5nVehn9OR"},"id":"ucG5nVehn9OR"},{"cell_type":"code","source":["# Import the necessary components from LangChain\n","from langchain_openai import ChatOpenAI  # OpenAI chat model wrapper\n","from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder  # Prompt templates\n","from langchain_core.runnables.history import RunnableWithMessageHistory  # Memory wrapper\n","from langchain_community.chat_message_histories import ChatMessageHistory  # In-memory storage\n","from langchain_core.chat_history import BaseChatMessageHistory  # Base class for history\n","\n","# Import the userdata module from google.colab to securely access user-specific data.\n","from google.colab import userdata\n","\n","# Retrieve the OpenAI API key from Colab's user data.\n","OPENAI_API_KEY = userdata.get('OPENAI_API_KEY')"],"metadata":{"id":"gJ5rlwaZYejt","executionInfo":{"status":"ok","timestamp":1762794984427,"user_tz":300,"elapsed":5449,"user":{"displayName":"David Smiley","userId":"04196970161563073370"}}},"id":"gJ5rlwaZYejt","execution_count":2,"outputs":[]},{"cell_type":"markdown","source":["### 1.2 - Initialize the Language Model"],"metadata":{"id":"B3eYVnOuoArq"},"id":"B3eYVnOuoArq"},{"cell_type":"code","execution_count":3,"id":"20ad6fe2","metadata":{"height":81,"tags":[],"id":"20ad6fe2","executionInfo":{"status":"ok","timestamp":1762794984843,"user_tz":300,"elapsed":415,"user":{"displayName":"David Smiley","userId":"04196970161563073370"}}},"outputs":[],"source":["# Initialize the language model with a fixed temperature (to ensure consistent responses)\n","llm = ChatOpenAI(temperature=0.0,\n","                 model=\"gpt-4o-mini\",\n","                 openai_api_key=OPENAI_API_KEY)"]},{"cell_type":"markdown","source":["### 1.3 - Set Up Session Storage"],"metadata":{"id":"yV_j_-nDoTBm"},"id":"yV_j_-nDoTBm"},{"cell_type":"code","source":["# Create a dictionary to store conversation histories\n","# In production, you would use a database (Redis, PostgreSQL, etc.)\n","# Key: session_id (string) → Value: ChatMessageHistory object\n","store = {}\n","\n","def get_session_history(session_id: str) -> BaseChatMessageHistory:\n","    \"\"\"\n","    Retrieve or create a chat history for a given session.\n","\n","    This function is called by RunnableWithMessageHistory to get the\n","    conversation history for a specific session_id. If the session\n","    doesn't exist, it creates a new ChatMessageHistory object.\n","\n","    Args:\n","        session_id: Unique identifier for the conversation session\n","\n","    Returns:\n","        ChatMessageHistory object containing the conversation history\n","\n","    IMPORTANT: In production, replace this with a database-backed solution:\n","        - Redis: Fast, in-memory storage for session data\n","        - PostgreSQL: Persistent storage with SQL queries\n","        - MongoDB: Document-based storage for JSON-like data\n","    \"\"\"\n","    if session_id not in store:\n","        store[session_id] = ChatMessageHistory()\n","    return store[session_id]"],"metadata":{"id":"7EZPq5rzoqYM","executionInfo":{"status":"ok","timestamp":1762794984846,"user_tz":300,"elapsed":2,"user":{"displayName":"David Smiley","userId":"04196970161563073370"}}},"id":"7EZPq5rzoqYM","execution_count":4,"outputs":[]},{"cell_type":"markdown","source":["### 1.4 - Create the Prompt Template"],"metadata":{"id":"4RKmrWQPo2QH"},"id":"4RKmrWQPo2QH"},{"cell_type":"code","source":["# Create a prompt template with a placeholder for conversation history\n","# The MessagesPlaceholder is where previous messages will be inserted\n","prompt = ChatPromptTemplate.from_messages([\n","    (\"system\", \"You are a helpful assistant.\"),  # System message sets behavior\n","    MessagesPlaceholder(variable_name=\"history\"),  # Previous conversation goes here\n","    (\"human\", \"{input}\")  # Current user input\n","])"],"metadata":{"id":"18CAWJlko1Wz","executionInfo":{"status":"ok","timestamp":1762794984865,"user_tz":300,"elapsed":15,"user":{"displayName":"David Smiley","userId":"04196970161563073370"}}},"id":"18CAWJlko1Wz","execution_count":5,"outputs":[]},{"cell_type":"markdown","source":["### 1.5 - Build the Chain with LCEL"],"metadata":{"id":"R3UbtzUyqSkN"},"id":"R3UbtzUyqSkN"},{"cell_type":"code","source":["# Create the chain using LCEL (LangChain Expression Language)\n","# The pipe operator (|) chains the prompt and model together\n","# Data flows: input → prompt (formats) → llm (generates) → output\n","chain = prompt | llm"],"metadata":{"id":"NahVh0eyYyav","executionInfo":{"status":"ok","timestamp":1762794984866,"user_tz":300,"elapsed":0,"user":{"displayName":"David Smiley","userId":"04196970161563073370"}}},"id":"NahVh0eyYyav","execution_count":6,"outputs":[]},{"cell_type":"markdown","source":["### 1.6 - Wrap Chain with Message History\n"],"metadata":{"id":"aw_7HAKKqZCH"},"id":"aw_7HAKKqZCH"},{"cell_type":"code","source":["# Wrap the chain with RunnableWithMessageHistory to enable memory\n","chain_with_history = RunnableWithMessageHistory(\n","    chain,                          # The chain to wrap\n","    get_session_history,            # Function to retrieve/create history\n","    input_messages_key=\"input\",     # Key in input dict that contains user message\n","    history_messages_key=\"history\"  # Key in prompt where history is inserted\n",")"],"metadata":{"id":"IEwVInoJqbSv","executionInfo":{"status":"ok","timestamp":1762794984881,"user_tz":300,"elapsed":14,"user":{"displayName":"David Smiley","userId":"04196970161563073370"}}},"id":"IEwVInoJqbSv","execution_count":7,"outputs":[]},{"cell_type":"markdown","source":["How it Works:\n","1. User calls chain_with_history.invoke({\"input\": \"...\"}, config={...})\n","2. RunnableWithMessageHistory extracts session_id from config\n","3. Calls get_session_history(session_id) to get conversation history\n","4. Inserts history into the \"history\" placeholder in the prompt\n","5. Runs the chain with the complete prompt\n","6. Saves the new exchange (user input + AI response) to history\n","7. Returns the response"],"metadata":{"id":"GgWK_TVIqY13"},"id":"GgWK_TVIqY13"},{"cell_type":"markdown","source":["### 1.7 - Have a Multi-Turn Conversation"],"metadata":{"id":"euXRYNk0qu4b"},"id":"euXRYNk0qu4b"},{"cell_type":"code","source":["# Conversation Turn #1: Introduction\n","# The user introduces themselves\n","chain_with_history.invoke(\n","    {\"input\": \"Hi, my name is David\"},\n","    config={\"configurable\": {\"session_id\": \"user_session_1\"}}\n",")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nMoBjr06Y7n1","executionInfo":{"status":"ok","timestamp":1762794987015,"user_tz":300,"elapsed":2133,"user":{"displayName":"David Smiley","userId":"04196970161563073370"}},"outputId":"c6c5d790-7b34-45dd-809b-354947fe2df9"},"id":"nMoBjr06Y7n1","execution_count":8,"outputs":[{"output_type":"execute_result","data":{"text/plain":["AIMessage(content='Hi David! How can I assist you today?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 10, 'prompt_tokens': 23, 'total_tokens': 33, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_560af6e559', 'id': 'chatcmpl-CaPmwI8Bu6Xfg9yxvPvNdgSFTiBEa', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--97ed0dca-1bd3-4734-814b-12b5045816d9-0', usage_metadata={'input_tokens': 23, 'output_tokens': 10, 'total_tokens': 33, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"]},"metadata":{},"execution_count":8}]},{"cell_type":"code","source":["# Conversation Turn #2: Simple Question\n","# Test that the model works for basic queries\n","chain_with_history.invoke(\n","    {\"input\": \"What is 1+1?\"},\n","    config={\"configurable\": {\"session_id\": \"user_session_1\"}}\n",")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ogYUB18oY7lJ","executionInfo":{"status":"ok","timestamp":1762794988527,"user_tz":300,"elapsed":1514,"user":{"displayName":"David Smiley","userId":"04196970161563073370"}},"outputId":"46ec609b-e135-43ed-86c4-9e6dabdcedc4"},"id":"ogYUB18oY7lJ","execution_count":9,"outputs":[{"output_type":"execute_result","data":{"text/plain":["AIMessage(content='1 + 1 equals 2.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 8, 'prompt_tokens': 48, 'total_tokens': 56, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_560af6e559', 'id': 'chatcmpl-CaPmyYFNqqmOOYGks60WbDPgFMeug', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--6f510782-3745-4796-8fe5-f9e2f892e464-0', usage_metadata={'input_tokens': 48, 'output_tokens': 8, 'total_tokens': 56, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"]},"metadata":{},"execution_count":9}]},{"cell_type":"code","source":["# Conversation Turn #3: Memory Test\n","# This tests whether the model remembers information from Turn #1\n","\n","chain_with_history.invoke(\n","    {\"input\": \"What is my name?\"},\n","    config={\"configurable\": {\"session_id\": \"user_session_1\"}}\n",")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hMNUMvZAY7iz","executionInfo":{"status":"ok","timestamp":1762794990384,"user_tz":300,"elapsed":1856,"user":{"displayName":"David Smiley","userId":"04196970161563073370"}},"outputId":"425db584-3c9b-4a59-fbeb-cf957a9b2f67"},"id":"hMNUMvZAY7iz","execution_count":10,"outputs":[{"output_type":"execute_result","data":{"text/plain":["AIMessage(content='Your name is David.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 5, 'prompt_tokens': 69, 'total_tokens': 74, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_560af6e559', 'id': 'chatcmpl-CaPn0C6znrtzSmdcUgTC21l4ilE8s', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--e90c7ece-a718-4c13-889e-829a5ae3c4d4-0', usage_metadata={'input_tokens': 69, 'output_tokens': 5, 'total_tokens': 74, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"]},"metadata":{},"execution_count":10}]},{"cell_type":"markdown","source":["### 1.8 - Inspect the Stored Conversation History"],"metadata":{"id":"jaRIV8ZarNWd"},"id":"jaRIV8ZarNWd"},{"cell_type":"code","source":["# Print the message history to show the complete conversation\n","history = store[\"user_session_1\"]\n","\n","# Iterate through all messages and display them\n","# Messages alternate: human → ai → human → ai → ...\n","for i, message in enumerate(history.messages, 1):\n","    # message.type is either \"human\" or \"ai\"\n","    # message.content is the actual text\n","    print(f\"{i}. {message.type.upper()}: {message.content}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nk0ShdMjY7ge","executionInfo":{"status":"ok","timestamp":1762794990388,"user_tz":300,"elapsed":3,"user":{"displayName":"David Smiley","userId":"04196970161563073370"}},"outputId":"0f57362f-a336-4de3-95bf-c7ca17e12c67"},"id":"nk0ShdMjY7ge","execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["1. HUMAN: Hi, my name is David\n","2. AI: Hi David! How can I assist you today?\n","3. HUMAN: What is 1+1?\n","4. AI: 1 + 1 equals 2.\n","5. HUMAN: What is my name?\n","6. AI: Your name is David.\n"]}]},{"cell_type":"markdown","source":["## 2. LangGraph with Checkpointers (Stateful Workflows)\n","Key Concepts:\n","- StateGraph: A graph where each node can read and modify state\n","- MessagesState: Built-in state schema that tracks conversation messages\n","- Checkpointer: Persistence layer that saves state snapshots\n","- Thread ID: Unique identifier for a conversation thread (like session_id)\n","- Nodes: Functions that process state and return updates\n","- Edges: Connections that define workflow between nodes\n","\n","\n","When to Use LangGraph:\n","- Complex multi-step workflows (agents, pipelines)\n","- Applications that need to branch based on conditions\n","- Systems requiring state inspection and debugging\n","- Production applications needing durable persistence\n","- Workflows with multiple tools or decision points\n","\n","When Not to Use:\n","- Simple single-turn Q&A (use basic chains instead)\n","- Rapid prototyping where complexity isn't needed\n","- Very simple chatbots without workflow logic\n","\n","Architecture:\n","Basic Chain (Section 1):\n","  User → Chain → LLM → Response\n","           ↓\n","      Session Store\n","\n","LangGraph (Section 2):\n","  User → Graph State → Node(s) → LLM → Update State → Response\n","                         ↓                    ↓\n","                    Checkpoint         Checkpoint\n","                    (before)           (after)\n","\n","\n","Key Differences: LangGraph saves the entire state at each step, enabling:\n","- Resume from any point\n","- Time-travel debugging\n","- Complex branching workflows\n","- Full state inspection"],"metadata":{"id":"ZQZHoFryZP7i"},"id":"ZQZHoFryZP7i"},{"cell_type":"markdown","source":["### 2.1 - Import LangGraph Components\n","\n","- StateGraph: The main class for building stateful graphs\n","- MessagesState: Pre-built state schema with \"messages\" key\n","- START: Special node representing the entry point to the graph\n","- InMemorySaver: Checkpointer that stores state in memory (for development)"],"metadata":{"id":"RBzuRFPhspCF"},"id":"RBzuRFPhspCF"},{"cell_type":"code","source":["from langgraph.graph import StateGraph, MessagesState, START\n","from langgraph.checkpoint.memory import InMemorySaver"],"metadata":{"id":"mrLrl8kvsxPk","executionInfo":{"status":"ok","timestamp":1762794990413,"user_tz":300,"elapsed":24,"user":{"displayName":"David Smiley","userId":"04196970161563073370"}}},"id":"mrLrl8kvsxPk","execution_count":12,"outputs":[]},{"cell_type":"markdown","source":["### 2.2 - Initialize the Checkpointer\n"],"metadata":{"id":"Xcblir_esyBI"},"id":"Xcblir_esyBI"},{"cell_type":"code","source":["# Create a checkpointer to enable persistent memory\n","# InMemorySaver stores checkpoints in RAM (lost when program stops)\n","checkpointer = InMemorySaver()"],"metadata":{"id":"rmixSUcZZPwC","executionInfo":{"status":"ok","timestamp":1762794990414,"user_tz":300,"elapsed":0,"user":{"displayName":"David Smiley","userId":"04196970161563073370"}}},"id":"rmixSUcZZPwC","execution_count":13,"outputs":[]},{"cell_type":"markdown","source":["### 2.3 - Define a Node Function\n"],"metadata":{"id":"_DeER2vWs-DU"},"id":"_DeER2vWs-DU"},{"cell_type":"code","source":["def call_model(state: MessagesState):\n","    \"\"\"\n","    Node that calls the LLM with the current conversation state.\n","\n","    In LangGraph, nodes are functions that:\n","    1. Receive the current state as input\n","    2. Perform some operation (call LLM, tool, etc.)\n","    3. Return a dictionary of state updates\n","\n","    Args:\n","        state: MessagesState containing \"messages\" key with conversation history\n","\n","    Returns:\n","        Dictionary with \"messages\" key containing the new AI response\n","\n","    STATE UPDATE MECHANISM:\n","    When you return {\"messages\": [response]}, LangGraph APPENDS the response\n","    to the existing messages list. This is because MessagesState uses the\n","    add_messages reducer, which concatenates new messages to the history.\n","    \"\"\"\n","    # Get all messages from state (includes full conversation history)\n","    messages = state[\"messages\"]\n","\n","    # Call the LLM with the complete message history\n","    response = llm.invoke(messages)\n","\n","    # Return the response as a state update\n","    # This will be appended to the messages list\n","    return {\"messages\": [response]}\n","\n","# Build the graph\n","builder = StateGraph(MessagesState)\n","builder.add_node(\"call_model\", call_model)\n","builder.add_edge(START, \"call_model\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"n2qeP625s8l7","executionInfo":{"status":"ok","timestamp":1762794990448,"user_tz":300,"elapsed":34,"user":{"displayName":"David Smiley","userId":"04196970161563073370"}},"outputId":"2d41b7c5-0d30-4042-c057-00bbfd132182"},"id":"n2qeP625s8l7","execution_count":14,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<langgraph.graph.state.StateGraph at 0x7de20eb8bd40>"]},"metadata":{},"execution_count":14}]},{"cell_type":"markdown","source":["### 2.4 - Build the State Graph"],"metadata":{"id":"ISNXts8htJZN"},"id":"ISNXts8htJZN"},{"cell_type":"code","source":["# Create a StateGraph with MessagesState schema\n","# MessagesState provides a \"messages\" key that stores conversation history\n","builder = StateGraph(MessagesState)\n","\n","# Add the call_model function as a node named \"call_model\"\n","# Node names are strings; they're used to reference nodes in edges\n","builder.add_node(\"call_model\", call_model)\n","\n","# Add an edge from START to call_model\n","# This means: when the graph starts, go to the call_model node\n","builder.add_edge(START, \"call_model\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9F7q-SLqtMio","executionInfo":{"status":"ok","timestamp":1762794990452,"user_tz":300,"elapsed":4,"user":{"displayName":"David Smiley","userId":"04196970161563073370"}},"outputId":"7140875a-e6df-46f0-ea9b-208bcb5e5399"},"id":"9F7q-SLqtMio","execution_count":15,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<langgraph.graph.state.StateGraph at 0x7de20ebbc8f0>"]},"metadata":{},"execution_count":15}]},{"cell_type":"markdown","source":["### 2.5 - Compile the Graph with Checkpointer\n","What Compile Does:\n","1. Validates the graph structure (no loops, all nodes reachable)\n","2. Optimizes execution order\n","3. Integrates the checkpointer for state persistence\n","4. Returns a runnable object with .invoke(), .stream(), etc.\n","\n","Checkpointer Behavior:\n","- Before executing: Loads previous state from checkpoint (if exists)\n","- After each node: Saves a new checkpoint with updated state\n","- Each checkpoint has a unique ID and timestamp\n","- You can \"time travel\" by loading any previous checkpoint\n"],"metadata":{"id":"yy6YEklLtUbZ"},"id":"yy6YEklLtUbZ"},{"cell_type":"code","source":["# Compile with checkpointer - this enables short-term memory\n","graph = builder.compile(checkpointer=checkpointer)"],"metadata":{"id":"c16eDu9atj-9","executionInfo":{"status":"ok","timestamp":1762794990453,"user_tz":300,"elapsed":2,"user":{"displayName":"David Smiley","userId":"04196970161563073370"}}},"id":"c16eDu9atj-9","execution_count":16,"outputs":[]},{"cell_type":"markdown","source":["### 2.6 - First Conversation in Thread 1\n","What Happens during Invoke:\n","1. Check if checkpoint exists for thread_1 (no - this is first message)\n","2. Initialize state: {\"messages\": [HumanMessage(\"Hi! I'm David\")]}\n","3. Save checkpoint (step -1)\n","4. Execute call_model node\n","5. Node returns: {\"messages\": [AIMessage(\"...\")]}\n","6. Merge into state: {\"messages\": [HumanMessage, AIMessage]}\n","7. Save checkpoint (step 0)\n","8. Return final state"],"metadata":{"id":"0uTltKJqtlOM"},"id":"0uTltKJqtlOM"},{"cell_type":"code","source":["# Configuration specifies which thread (conversation) to use\n","# thread_id is like session_id but for stateful workflows\n","config1 = {\"configurable\": {\"thread_id\": \"thread_1\"}}\n","\n","# First interaction: User introduces themselves\n","user_content = \"Hi! I'm David\"\n","response1 = graph.invoke(\n","    {\"messages\": [{\"role\": \"user\", \"content\": user_content}]},\n","    config1\n",")\n","\n","print(f\"User: {response1['messages'][-2].content}\")  # -2 = second to last (user message)\n","print(f\"Assistant: {response1['messages'][-1].content}\")  # -1 = last (AI response)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7cuZEZWjZPtu","executionInfo":{"status":"ok","timestamp":1762794992221,"user_tz":300,"elapsed":1769,"user":{"displayName":"David Smiley","userId":"04196970161563073370"}},"outputId":"485dd2e8-585c-416a-9437-87a69a5c5d66"},"id":"7cuZEZWjZPtu","execution_count":17,"outputs":[{"output_type":"stream","name":"stdout","text":["User: Hi! I'm David\n","Assistant: Hi David! How can I assist you today?\n"]}]},{"cell_type":"markdown","source":["### 2.7 - Continue Conversation in Same Thread\n","What Happens during Second Invoke:\n","1. Check if checkpoint exists for thread_1 (YES - load it)\n","2. Load previous state: {\"messages\": [HumanMessage(\"Hi! I'm David\"), AIMessage(\"...\")]}\n","3. Merge new input: {\"messages\": [...previous..., HumanMessage(\"What's my name?\")]}\n","4. Save checkpoint\n","5. Execute call_model node with FULL history\n","6. Node sees all 3 messages and can reference \"David\" from first message\n","7. Save final checkpoint\n","8. Return state with all 4 messages"],"metadata":{"id":"s3nK9tC9uA8S"},"id":"s3nK9tC9uA8S"},{"cell_type":"code","source":["# Second interaction in same thread: Test if model remembers name\n","user_content = \"What's my name?\"\n","response2 = graph.invoke(\n","    {\"messages\": [{\"role\": \"user\", \"content\": user_content}]},\n","    config1  # SAME thread_id as before\n",")\n","\n","print(f\"User: {response2[\"messages\"][-2].content}\")\n","print(f\"Assistant: {response2['messages'][-1].content}\\n\")\n","print(f\"\\nTotal messages in thread_1: {len(response2['messages'])}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RbuR0SH0ZPiV","executionInfo":{"status":"ok","timestamp":1762794994065,"user_tz":300,"elapsed":1844,"user":{"displayName":"David Smiley","userId":"04196970161563073370"}},"outputId":"e009cce2-77a4-489d-f575-4b511c5b7aff"},"id":"RbuR0SH0ZPiV","execution_count":18,"outputs":[{"output_type":"stream","name":"stdout","text":["User: What's my name?\n","Assistant: Your name is David! How can I help you today?\n","\n","\n","Total messages in thread_1: 4\n"]}]},{"cell_type":"markdown","source":["### 2.8 - New Thread in Isolated Memory\n","What Happens in New Thread:\n","1. Check if checkpoint exists for thread_2 (NO - new thread)\n","2. Initialize fresh state: {\"messages\": [HumanMessage(\"What's my name?\")]}\n","3. Execute call_model node\n","4. Node only sees ONE message (no previous context)\n","5. AI cannot answer because it has no previous context\n","6. Save checkpoint for thread_2\n"],"metadata":{"id":"5z3DPHJZucqw"},"id":"5z3DPHJZucqw"},{"cell_type":"code","source":["user_content = \"What's my name?\"\n","\n","# New thread - memory is isolated\n","# Second conversation in thread \"thread_2\"\n","config2 = {\"configurable\": {\"thread_id\": \"thread_2\"}}\n","\n","response3 = graph.invoke(\n","    {\"messages\": [{\"role\": \"user\", \"content\": user_content}]},\n","    config2 # DIFFERENT thread_id\n",")\n","print(f\"Thread 2 - User: {response3['messages'][-2].content}\")\n","print(f\"Assistant: {response3['messages'][-1].content}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jGxrIQPdaxdS","executionInfo":{"status":"ok","timestamp":1762794996316,"user_tz":300,"elapsed":2250,"user":{"displayName":"David Smiley","userId":"04196970161563073370"}},"outputId":"62084106-6ae6-45cc-c52b-b9eebf75654e"},"id":"jGxrIQPdaxdS","execution_count":19,"outputs":[{"output_type":"stream","name":"stdout","text":["Thread 2 - User: What's my name?\n","Assistant: I'm sorry, but I don't have access to personal information about you unless you've shared it in this conversation. How can I assist you today?\n"]}]},{"cell_type":"markdown","source":["### 2.9: Return to Thread 1 for Memory Persistence\n"],"metadata":{"id":"CLcZLfTrutvu"},"id":"CLcZLfTrutvu"},{"cell_type":"code","source":["user_content = \"What's my name?\"\n","\n","# Continue conversation in same thread\n","response2 = graph.invoke(\n","    {\"messages\": [{\"role\": \"user\", \"content\": user_content}]},\n","    config1\n",")\n","print(f\"User: {response2[\"messages\"][-2].content}\")\n","print(f\"Assistant: {response2['messages'][-1].content}\\n\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MOTM2OXfaKoN","executionInfo":{"status":"ok","timestamp":1762794998053,"user_tz":300,"elapsed":1724,"user":{"displayName":"David Smiley","userId":"04196970161563073370"}},"outputId":"ccca90c7-2376-481e-abe1-ba4353d12adf"},"id":"MOTM2OXfaKoN","execution_count":20,"outputs":[{"output_type":"stream","name":"stdout","text":["User: What's my name?\n","Assistant: Your name is David. If there's anything else you'd like to discuss or ask, feel free!\n","\n"]}]},{"cell_type":"markdown","source":["1. Thread Isolation:\n","   - Each thread_id has its own separate conversation history\n","   - Changes in one thread don't affect other threads\n","   - Like having multiple separate chat windows\n","\n","2. Checkpoint Persistence:\n","   - State is automatically saved after each step\n","   - Can resume conversations at any time\n","   - Full history is preserved across invocations\n","\n","3. State Accumulation:\n","   - Messages accumulate in the state with each interaction\n","   - Each invoke adds to the existing history (doesn't replace)\n","   - Full context is available to the model\n","\n","4. LangGraph vs Basic Chains:\n","   - Basic chains: Session-based, simpler API\n","   - LangGraph: Thread-based, full state control, better for complex workflows"],"metadata":{"id":"fRvsalRQu1mq"},"id":"fRvsalRQu1mq"},{"cell_type":"markdown","source":["## 3. Trim Messages (Managing Context Window)\n","The Context Windown Problem:\n","- GPT-4o-mini: ~128K tokens context window\n","- Long conversation: May have 50+ exchanges = potentially 100K+ tokens\n","- What happens when you exceed the limit?\n","  → Error: \"This model's maximum context length is X tokens...\"\n","  → Solution: Trim messages to stay under the limit\n","\n","Key Concepts:\n","- Context Window: Maximum number of tokens the LLM can process at once\n","- Trimming: Keeping only the N most recent messages\n","- RemoveMessage: Special message type that deletes messages from state\n","- Permanent Deletion: Unlike just reading fewer messages, we delete from checkpoint\n","\n","Trimming Strategies:\n","1. Keep last N messages (what we'll demonstrate)\n","2. Keep last N tokens (more precise but requires token counting)\n","3. Keep first message + last N messages (preserves system prompt)\n","4. Smart trimming (keep important messages, remove filler)\n","\n","When to Use Trimming:\n","- Long-running conversations (customer support, tutoring)\n","- When recent context is most important\n","- When you want to control memory usage\n","- When conversation history isn't critical\n","\n","Tradeoffs:\n","- Pros: Stays within context limits, reduces token costs, faster processing\n","- Cons: Loses conversation history, may forget important early context\n","\n","Comparison with Other Strategies:\n","- Trimming: Fast, simple, but loses information\n","- Summarization (Section 4): Preserves key info but costs more tokens\n","- External memory: Store full history in database, retrieve relevant parts"],"metadata":{"id":"f5jQWA1Fck0h"},"id":"f5jQWA1Fck0h"},{"cell_type":"markdown","source":["### 3.1 - Import RemoveMessage\n","- RemoveMessage is a special message type that signals deletion\n","- When you return {\"messages\": [RemoveMessage(id=msg_id)]}, LangGraph removes that message from the state\n","\n","IMPORTANT: RemoveMessage only works with MessagesState because MessagesState uses the add_messages reducer which handles RemoveMessage specially."],"metadata":{"id":"TB5OgTrcvZLL"},"id":"TB5OgTrcvZLL"},{"cell_type":"code","source":["from langchain_core.messages import trim_messages, RemoveMessage"],"metadata":{"id":"Uy2HecObvn8d","executionInfo":{"status":"ok","timestamp":1762794998093,"user_tz":300,"elapsed":27,"user":{"displayName":"David Smiley","userId":"04196970161563073370"}}},"id":"Uy2HecObvn8d","execution_count":21,"outputs":[]},{"cell_type":"markdown","source":["### 3.2 - Define Node with Trimming Logic\n"],"metadata":{"id":"H0CXGah4vbyu"},"id":"H0CXGah4vbyu"},{"cell_type":"code","source":["def call_model_with_trimming(state: MessagesState):\n","    \"\"\"\n","    Node that trims messages before calling model AND updates state.\n","\n","    This function implements a simple trimming strategy:\n","    - Keep only the last 4 messages (2 user + 2 assistant exchanges)\n","    - Permanently delete older messages from the state\n","    - Call the LLM with only the recent context\n","\n","    TRIMMING LOGIC BREAKDOWN:\n","    1. Check if we have more than 4 messages\n","    2. If yes: Keep last 4, create RemoveMessage for the rest\n","    3. Call LLM with trimmed context\n","    4. Return deletions + new response\n","\n","    Args:\n","        state: MessagesState with full conversation history\n","\n","    Returns:\n","        Dictionary with messages to delete and new AI response\n","    \"\"\"\n","    messages = state[\"messages\"]\n","\n","    # Only keep last 4 messages (2 exchanges) to demonstrate trimming\n","    if len(messages) > 4:\n","        # Trim to keep only recent messages\n","        trimmed = messages[-4:]\n","\n","        # Delete old messages from state permanently\n","        messages_to_delete = [RemoveMessage(id=m.id) for m in messages[:-4]]\n","\n","        # Call model with trimmed messages\n","        response = llm.invoke(trimmed)\n","\n","        # Return both the new response and deletions\n","        return {\"messages\": messages_to_delete + [response]}\n","    else:\n","        # Not enough messages to trim yet\n","        response = llm.invoke(messages)\n","        return {\"messages\": [response]}"],"metadata":{"id":"zKljSZYdaKl_","executionInfo":{"status":"ok","timestamp":1762794998110,"user_tz":300,"elapsed":1,"user":{"displayName":"David Smiley","userId":"04196970161563073370"}}},"id":"zKljSZYdaKl_","execution_count":22,"outputs":[]},{"cell_type":"markdown","source":["### 3.3 - Build Graph with Trimming"],"metadata":{"id":"8zcE7qwXvx8y"},"id":"8zcE7qwXvx8y"},{"cell_type":"code","source":["# Build a new graph with our trimming node\n","trim_builder = StateGraph(MessagesState)\n","trim_builder.add_node(\"call_model\", call_model_with_trimming)\n","trim_builder.add_edge(START, \"call_model\")\n","\n","# Compile with a NEW checkpointer (separate from previous examples)\n","trim_graph = trim_builder.compile(checkpointer=InMemorySaver())\n","\n","# Use a unique thread_id for this demonstration\n","config = {\"configurable\": {\"thread_id\": \"trim_thread\"}}"],"metadata":{"id":"6s2K1XykaKjz","executionInfo":{"status":"ok","timestamp":1762794998112,"user_tz":300,"elapsed":1,"user":{"displayName":"David Smiley","userId":"04196970161563073370"}}},"id":"6s2K1XykaKjz","execution_count":23,"outputs":[]},{"cell_type":"markdown","source":["### 3.4 - Build Up Conversation History"],"metadata":{"id":"U0Goui-Mv5kY"},"id":"U0Goui-Mv5kY"},{"cell_type":"code","source":["print(\"Building up conversation history...\")\n","# Exchange 1: User introduces themselves\n","trim_graph.invoke({\"messages\": [{\"role\": \"user\", \"content\": \"Hi, my name is David\"}]}, config)\n","\n","# Exchange 2: Discuss Python\n","trim_graph.invoke({\"messages\": [{\"role\": \"user\", \"content\": \"I love Python programming\"}]}, config)\n","\n","# Exchange 3: Discuss teaching\n","trim_graph.invoke({\"messages\": [{\"role\": \"user\", \"content\": \"I also enjoy teaching\"}]}, config)\n","\n","# Exchange 4: Request a poem\n","trim_graph.invoke({\"messages\": [{\"role\": \"user\", \"content\": \"Write me a poem about NLP\"}]}, config)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0oJYTfS-aKha","executionInfo":{"status":"ok","timestamp":1762795010226,"user_tz":300,"elapsed":12114,"user":{"displayName":"David Smiley","userId":"04196970161563073370"}},"outputId":"1711b8e0-cbb8-4b99-b7d7-15c0d65e6bcd"},"id":"0oJYTfS-aKha","execution_count":24,"outputs":[{"output_type":"stream","name":"stdout","text":["Building up conversation history...\n"]},{"output_type":"execute_result","data":{"text/plain":["{'messages': [AIMessage(content=\"That's great to hear, David! Python is a versatile and powerful programming language. What do you enjoy most about it? Are you working on any specific projects or learning something new?\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 36, 'prompt_tokens': 35, 'total_tokens': 71, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_560af6e559', 'id': 'chatcmpl-CaPnAD956YM9Q7I4j2eQMsE789gg5', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--3fb354e8-ece6-4168-9ddf-99b2d54a68e4-0', usage_metadata={'input_tokens': 35, 'output_tokens': 36, 'total_tokens': 71, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}),\n","  HumanMessage(content='I also enjoy teaching', additional_kwargs={}, response_metadata={}, id='8797138e-b73e-4054-ac8b-15da594c9520'),\n","  AIMessage(content=\"That's wonderful! Teaching can be incredibly rewarding, especially when you get to share your passion for Python with others. Do you teach programming formally, or do you help others learn in a more informal setting? What topics do you usually cover?\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 47, 'prompt_tokens': 73, 'total_tokens': 120, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_560af6e559', 'id': 'chatcmpl-CaPnCEwLH8hLstigQZFBrGLuk7gGs', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--9d7cb49a-311a-438c-92c7-9ce49eb41591-0', usage_metadata={'input_tokens': 73, 'output_tokens': 47, 'total_tokens': 120, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}),\n","  HumanMessage(content='Write me a poem about NLP', additional_kwargs={}, response_metadata={}, id='bd9a1687-e373-451f-8367-80f1b6170489'),\n","  AIMessage(content='In the realm where words take flight,  \\nA dance of language, pure delight,  \\nFrom whispers soft to shouts so loud,  \\nNLP weaves through the digital crowd.  \\n\\nWith tokens parsed and meanings sought,  \\nMachines now grasp the thoughts we’ve wrought,  \\nFrom syntax trees to semantic nets,  \\nA tapestry of language, no regrets.  \\n\\nSentiment sways like a gentle breeze,  \\nUnderstanding hearts with effortless ease,  \\nChatbots converse, with wit and charm,  \\nBridging the gap, they mean no harm.  \\n\\nIn data’s depths, where patterns hide,  \\nAlgorithms learn, and knowledge bides,  \\nTransformers rise, with attention keen,  \\nCrafting responses, both sharp and serene.  \\n\\nFrom translation’s grace to summarization,  \\nNLP fuels our communication,  \\nIn every byte, a story unfolds,  \\nA symphony of language, timeless and bold.  \\n\\nSo here’s to the art of words and code,  \\nTo the bridges we build on this winding road,  \\nIn the world of NLP, let’s explore,  \\nThe magic of language, forevermore.  ', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 229, 'prompt_tokens': 112, 'total_tokens': 341, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_51db84afab', 'id': 'chatcmpl-CaPnGZTrFQJ5VfFFW7kKbZiu4rPgb', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--61f8a26f-f698-497d-81ad-275af16cf42e-0', usage_metadata={'input_tokens': 112, 'output_tokens': 229, 'total_tokens': 341, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]}"]},"metadata":{},"execution_count":24}]},{"cell_type":"markdown","source":["### 3.5 - Test Memory After Trimming"],"metadata":{"id":"UOGhjWSrwFA-"},"id":"UOGhjWSrwFA-"},{"cell_type":"code","source":["# Now ask about early conversation - it will be trimmed out\n","response = trim_graph.invoke({\"messages\": [{\"role\": \"user\", \"content\": \"What's my name?\"}]}, config)\n","print(f\"\\nAfter 4 exchanges, asking: What's my name?\")\n","print(f\"Assistant: {response['messages'][-1].content}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"K8rXEj9HaKe9","executionInfo":{"status":"ok","timestamp":1762795014865,"user_tz":300,"elapsed":4637,"user":{"displayName":"David Smiley","userId":"04196970161563073370"}},"outputId":"17afc696-bcb9-4b89-c63d-a22ff218ab06"},"id":"K8rXEj9HaKe9","execution_count":25,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","After 4 exchanges, asking: What's my name?\n","Assistant: I'm sorry, but I don't have access to personal information about users unless you share it with me during our conversation. If you'd like to tell me your name, I'd be happy to use it!\n"]}]},{"cell_type":"markdown","source":["### 3.6 - Verify State Contents"],"metadata":{"id":"WEDXPOFAwIcv"},"id":"WEDXPOFAwIcv"},{"cell_type":"code","source":["# Get the current state to see what's actually stored\n","trim_state = trim_graph.get_state(config)\n","print(f\"\\nTotal messages in checkpoint: {len(trim_state.values['messages'])}\")\n","print(\"\\nActual messages stored:\")\n","\n","for i, msg in enumerate(trim_state.values['messages'], 1):\n","    print(f\"  {i}. {msg.type}: {msg.content[:70]}...\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wKq68RVgwLsm","executionInfo":{"status":"ok","timestamp":1762795014887,"user_tz":300,"elapsed":20,"user":{"displayName":"David Smiley","userId":"04196970161563073370"}},"outputId":"11e268ac-1017-4d51-89e2-0492e348dc1e"},"id":"wKq68RVgwLsm","execution_count":26,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Total messages in checkpoint: 5\n","\n","Actual messages stored:\n","  1. ai: That's wonderful! Teaching can be incredibly rewarding, especially whe...\n","  2. human: Write me a poem about NLP...\n","  3. ai: In the realm where words take flight,  \n","A dance of language, pure deli...\n","  4. human: What's my name?...\n","  5. ai: I'm sorry, but I don't have access to personal information about users...\n"]}]},{"cell_type":"markdown","source":["1. Permanent Deletion:\n","   - RemoveMessage permanently deletes from checkpoint\n","   - Different from just reading fewer messages\n","   - Reduces memory/storage usage\n","\n","2. Context Window Management:\n","   - Keeps conversation within token limits\n","   - Prevents \"context length exceeded\" errors\n","   - Makes responses faster (less to process)\n","\n","3. Tradeoffs: Efficiency vs Memory:\n","   - Gains: Faster, cheaper, stays within limits\n","   - Cost: Loses conversation history, may forget important info\n","\n","4. When to Use:\n","   - Long-running conversations\n","   - When recent context matters most\n","   - When you need strict token control\n","   - Customer support (recent issue more important than history)\n","\n","5. Alternatives to Consider:\n","   - Summarization (On Thursday): Keeps compressed history\n","   - External memory: Store full history, retrieve relevant parts\n","   - Smart trimming: Keep important messages, remove filler"],"metadata":{"id":"XRtJr9rxwQdi"},"id":"XRtJr9rxwQdi"}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.17"},"colab":{"provenance":[],"toc_visible":true}},"nbformat":4,"nbformat_minor":5}